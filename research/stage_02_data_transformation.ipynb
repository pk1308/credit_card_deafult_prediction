{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\princ\\\\OneDrive\\\\Desktop\\\\project-python\\\\creditcard'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_current_time_stamp():\n",
    "    return f\"{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "\n",
    "ROOT_DIR = os.getcwd()  # to get current working directory\n",
    "CURRENT_TIME_STAMP = get_current_time_stamp()\n",
    "# config constants\n",
    "CONFIG_DIR = os.path.join(ROOT_DIR, 'configs')\n",
    "CONFIG_FILE_NAME = \"config.yaml\"\n",
    "CONFIG_FILE_PATH = os.path.join(CONFIG_DIR, CONFIG_FILE_NAME)\n",
    "\n",
    "\n",
    "DATABASE_FILE_NAME = \"database.yaml\"\n",
    "DATABASE_FILE = Path(os.path.join(CONFIG_DIR , DATABASE_FILE_NAME))\n",
    "\n",
    "FEATURE_GENERATOR_FILE_NAME = \"feature_generator.yaml\"\n",
    "FEATURE_GENERATOR_FILE_PATH = Path(os.path.join(CONFIG_DIR ,FEATURE_GENERATOR_FILE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel, DirectoryPath, FilePath\n",
    "\n",
    "\n",
    "class DataIngestionConfig(BaseModel):\n",
    "    dataset_download_id: str\n",
    "    raw_data_file_path: Path\n",
    "    ingested_train_file_path: Path\n",
    "    ingested_test_data_path: Path\n",
    "    random_state : int\n",
    "\n",
    "\n",
    "class TrainingPipelineConfig(BaseModel):\n",
    "    artifact_dir: DirectoryPath\n",
    "    pipeline_name: str\n",
    "    experiment_code : str\n",
    "    training_random_state : int\n",
    "\n",
    "\n",
    "class DataValidationConfig(BaseModel):\n",
    "    experiment_code: str\n",
    "    data_validated_artifact_dir: DirectoryPath\n",
    "    schema_file_path: FilePath\n",
    "    report_file_dir: Path\n",
    "    data_validated_test_collection: str\n",
    "    data_validated_train_collection: str\n",
    "    \n",
    "class DataTransformationConfig(BaseModel):\n",
    "    data_validated_train_collection: str\n",
    "    schema_file_path: FilePath\n",
    "    random_state : int\n",
    "    preprocessed_object_file_path : Path\n",
    "    to_train_collection : str\n",
    "    to_test_collection : str\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel, DirectoryPath, FilePath\n",
    "\n",
    "\n",
    "class DataIngestionArtifact(BaseModel):\n",
    "    train_file_path: FilePath\n",
    "    test_file_path: FilePath\n",
    "\n",
    "\n",
    "class DataValidationArtifact(BaseModel):\n",
    "    schema_file_path : FilePath\n",
    "    report_file_dir : DirectoryPath\n",
    "    is_validated : bool\n",
    "    \n",
    "class  DataTransformationArtifact (BaseModel):\n",
    "    transformed_train_file_path : FilePath\n",
    "    transformed_test_file_path : FilePath\n",
    "    preprocessed_object_file_path : FilePath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from kneed import KneeLocator\n",
    "from sklearn import set_config\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from CreditCard.entity import DataTransformationArtifact, DataTransformationConfig\n",
    "from CreditCard.logging import logger\n",
    "from CreditCard.exception import AppException\n",
    "from CreditCard.utils import read_yaml , save_bin\n",
    "from CreditCard.Database import MongoDB\n",
    "from CreditCard.constants import FEATURE_GENERATOR_FILE_PATH\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "class FeatureGenerator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"custom feature generator class to generate cluster class for the data\n",
    "    scaler : StandardScaler clustering using kmeans++ and kneed\"\"\"\n",
    "\n",
    "    def __init__(self, feature_generator_file_path : Path =FEATURE_GENERATOR_FILE_PATH):\n",
    "        try:\n",
    "            self.feature_config_info = read_yaml(path_to_yaml= feature_generator_file_path)\n",
    "            self.cluster = None\n",
    "            self.pay_x = self.feature_config_info.pay_x_columns\n",
    "            self.age = self.feature_config_info.Age_column\n",
    "            self.bill_amt = self.feature_config_info.bill_amt_columns\n",
    "            self.pay_amt_columns = self.feature_config_info.pay_amt_columns\n",
    "            self.limit_column = self.feature_config_info.limit_column\n",
    "            self.encoder = OneHotEncoder(sparse=False)\n",
    "            self.imputer = SimpleImputer()\n",
    "\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        data_imputed = self.imputer.fit_transform(X)\n",
    "        data_generated = self.prepare_data(X=data_imputed)\n",
    "        data_encoded = self.encoder.fit_transform(data_generated)\n",
    "        wcss=[]\n",
    "        for i in range(1,11):\n",
    "            kmeans=KMeans(n_clusters=i, init='k-means++',random_state=42)\n",
    "            kmeans.fit(data_encoded)\n",
    "            wcss.append(kmeans.inertia_) \n",
    "\n",
    "        kn = KneeLocator(range(1, 11), wcss, curve='convex', direction='decreasing')\n",
    "        total_clusters=kn.knee\n",
    "        logger.info(f\"total cluster :{total_clusters}\")\n",
    "        self.cluster = KMeans(n_clusters=total_clusters, init='k-means++',random_state=42)\n",
    "        self.cluster.fit(data_encoded)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        try:\n",
    "            data_imputed = self.imputer.transform(X)\n",
    "            data_generated = self.prepare_data(X=data_imputed)\n",
    "            data_encoded = self.encoder.transform(data_generated)\n",
    "            data_generated[\"cluster\"]  = self.cluster.predict(data_encoded)\n",
    "            data_encoded[\"cluster\"] = data_generated[\"cluster\"]\n",
    "            return data_encoded\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "        \n",
    "    def prepare_data(self, X, y=None):\n",
    "        try:\n",
    "            feature_config_bin_info = self.feature_config_info.bins\n",
    "            data_generated = pd.DataFrame()\n",
    "            pay_feature_ceil = feature_config_bin_info.pay_feature_ceil\n",
    "            pay_feature = lambda x: x if x < pay_feature_ceil else pay_feature_ceil\n",
    "            for col in self.pay_x:\n",
    "                data_generated[col] = X[col].apply(pay_feature)\n",
    "            age_bin = feature_config_bin_info.age_bins\n",
    "            data_generated[self.age[0]]= pd.cut(X[self.age[0]],age_bin)\n",
    "            bill_amount_bins = feature_config_bin_info.bill_amount_bins\n",
    "            for col in self.bill_amt:\n",
    "                data_generated[col] = pd.cut(X[col],bill_amount_bins)\n",
    "            pay_amt_bins = feature_config_bin_info.pay_amt_bins\n",
    "            for col in self.pay_amt_columns:\n",
    "                data_generated[col] = pd.cut(X[col],pay_amt_bins)\n",
    "            limit_bins = feature_config_bin_info.limit_bins\n",
    "            data_generated[self.limit_column[0]] =pd.cut(X[self.limit_column[0]],limit_bins)\n",
    "            [data_generated[col].astype(\"category\")for col in data_generated.columns]\n",
    "            return data_generated\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "\n",
    "class DataTransformation:\n",
    "    \"\"\"Data transformation class . Choose the columns to model and transform the data\"\"\"\n",
    "\n",
    "    def __init__(self, data_transformation_config: DataTransformationConfig):\n",
    "        try:\n",
    "            self.data_transformation_config_info = data_transformation_config\n",
    "            train_collections = self.data_transformation_config_info.data_validated_train_collection\n",
    "            self.train_connection = MongoDB(train_collections, drop_collection=False)\n",
    "            self.train_df = self.train_connection.find_many_as_df()\n",
    "            to_train_collection = self.data_transformation_config_info.to_train_collection\n",
    "            to_test_collection = self.data_transformation_config_info.to_test_collection\n",
    "            self.to_train_connection = MongoDB(to_train_collection , drop_collection=False)\n",
    "            self.to_test_connection = MongoDB(to_test_collection , drop_collection=False)\n",
    "            logger.info(f\"{'>>' * 30}Data Transformation log started.{'<<' * 30} \")\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "\n",
    "    def get_data_transformer_object(self) -> ColumnTransformer:\n",
    "        try:\n",
    "            preprocessing = Pipeline(steps=[('feature_generator', FeatureGenerator())])\n",
    "            return preprocessing\n",
    "\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "        \n",
    "    def initiate_data_transformation(self) -> DataTransformationArtifact:\n",
    "        try:\n",
    "            logger.info(\"Obtaining preprocessing object.\")\n",
    "            preprocessing_obj = self.get_data_transformer_object()\n",
    "            data_transformation_config_info =  self.data_transformation_config_info\n",
    "            schema_file_path = self.data_transformation_config_info.schema_file_path\n",
    "\n",
    "            dataset_schema = read_yaml(path_to_yaml=schema_file_path)\n",
    "            target_column_name = dataset_schema.target_column\n",
    "            logger.info(f\"Target column name : {target_column_name}\")\n",
    "            logger.info(\"Splitting input and target feature from training and testing dataframe.\")\n",
    "            input_feature_train_df = self.train_df.drop(columns=[target_column_name], axis=1)\n",
    "            target_feature_train_df = self.train_df[target_column_name]\n",
    "            random_state = data_transformation_config_info.random_state\n",
    "            X_train, X_test, y_train, y_test = train_test_split( input_feature_train_df, target_feature_train_df, test_size=0.20, random_state=random_state)\n",
    "           \n",
    "            \n",
    "            logger.info(\"Applying preprocessing object on training dataframe and testing dataframe\")\n",
    "            to_train_df = preprocessing_obj.fit_transform(X_train)\n",
    "            to_test_df = preprocessing_obj.fit_transform(X_test)\n",
    "        \n",
    "            \n",
    "            transformed_train_file_path = data_transformation_config_info.transformed_train_file_path\n",
    "            transformed_test_file_path = data_transformation_config_info.transformed_test_file_path\n",
    "            \n",
    "            to_test_df[target_column_name] = y_test.copy(deep= True)\n",
    "            to_train_df[target_column_name] = y_train.copy(deep=True)\n",
    "            logger.info(\"Saving transformed training and testing array.\")\n",
    "            preprocessing_obj_file_path = self. data_transformation_config_info.preprocessed_object_file_path\n",
    "            self.to_test_connection.insert_many_(to_test_df.to_dict(orient=\"records\"))\n",
    "            self.to_train_connection.insert_many_(to_train_df.to_dict(orient=\"records\"))\n",
    "            logger.info(\"Saving preprocessing object.\")\n",
    "            save_bin(file_path=preprocessing_obj_file_path, obj=preprocessing_obj)\n",
    "\n",
    "            data_transformation_artifact = DataTransformationArtifact(transformed_train_file_path=transformed_train_file_path,\n",
    "                                                                      transformed_test_file_path=transformed_test_file_path,\n",
    "                                                                      preprocessed_object_file_path=preprocessing_obj_file_path)\n",
    "            logger.info(f\"Data transformations artifact: {data_transformation_artifact}\")\n",
    "            return data_transformation_artifact\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "\n",
    "    def __del__(self):\n",
    "        logger.info(f\"{'>>' * 30}Data Transformation log completed.{'<<' * 30} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import  os\n",
    "import  json\n",
    "\n",
    "from CreditCard.entity import DataIngestionConfig ,  TrainingPipelineConfig , DataValidationConfig , DataTransformationConfig\n",
    "from CreditCard.exception import AppException\n",
    "from CreditCard.logging import logger\n",
    "from CreditCard.utils import read_yaml , create_directories\n",
    "from pathlib import Path\n",
    "from CreditCard.constants import CONFIG_FILE_PATH ,  CURRENT_TIME_STAMP , ROOT_DIR , CONFIG_DIR\n",
    "\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "\n",
    "    def __init__(self,\n",
    "                 config_file_path: Path = CONFIG_FILE_PATH) -> None:\n",
    "        try:\n",
    "            self.config_info = read_yaml(path_to_yaml=Path(config_file_path))\n",
    "            self.pipeline_config = self.get_training_pipeline_config()\n",
    "            self.time_stamp = CURRENT_TIME_STAMP\n",
    "\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \n",
    "        try:\n",
    "            data_ingestion_info = self.config_info.data_ingestion_config\n",
    "            pipeline_config = self.pipeline_config\n",
    "            artifact_dir = pipeline_config.artifact_dir\n",
    "            experiment_code = pipeline_config.experiment_code\n",
    "            random_state = pipeline_config.training_random_state\n",
    "            dataset_download_id = data_ingestion_info.dataset_download_id\n",
    "            data_ingestion_dir_name = data_ingestion_info.ingestion_dir\n",
    "            raw_data_dir = data_ingestion_info.raw_data_dir\n",
    "            raw_file_name = data_ingestion_info.dataset_download_file_name\n",
    "            data_ingestion_dir = os.path.join(artifact_dir, data_ingestion_dir_name, experiment_code)\n",
    "            raw_data_file_path  = os.path.join(data_ingestion_dir, raw_data_dir, raw_file_name)\n",
    "            ingested_dir_name = data_ingestion_info.ingested_dir\n",
    "            ingested_dir_path = os.path.join(data_ingestion_dir,ingested_dir_name)\n",
    "            \n",
    "            ingested_train_file_path  = os.path.join(ingested_dir_path, data_ingestion_info.ingested_train_file)\n",
    "            ingested_test_file_path = os.path.join(ingested_dir_path, data_ingestion_info.ingested_test_file)\n",
    "            create_directories([os.path.dirname(raw_data_file_path), os.path.dirname(ingested_train_file_path)])\n",
    "            \n",
    "            data_ingestion_config = DataIngestionConfig(dataset_download_id = dataset_download_id , \n",
    "                                                        raw_data_file_path = raw_data_file_path , \n",
    "                                                        ingested_train_file_path = ingested_train_file_path , \n",
    "                                                        ingested_test_data_path  = ingested_test_file_path,\n",
    "                                                        random_state = random_state)\n",
    "            \n",
    "            return data_ingestion_config\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "    def get_training_pipeline_config(self) -> TrainingPipelineConfig:\n",
    "        try:\n",
    "            training_config = self.config_info.training_pipeline_config\n",
    "            training_pipeline_name = training_config.pipeline_name\n",
    "            training_experiment_code = training_config.experiment_code\n",
    "            training_random_state = training_config.random_state\n",
    "            training_artifacts = os.path.join(ROOT_DIR, training_config.artifact_dir )\n",
    "            create_directories(path_to_directories = [training_artifacts])\n",
    "            training_pipeline_config =  TrainingPipelineConfig(artifact_dir=training_artifacts ,\n",
    "                                                               experiment_code = training_experiment_code,\n",
    "                                                               pipeline_name=training_pipeline_name,\n",
    "                                                               training_random_state=training_random_state)\n",
    "            logger.info(f\"Training pipeline config: {training_pipeline_config}\")\n",
    "            return training_pipeline_config\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "        \n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        try:\n",
    "            pipeline_config = self.pipeline_config\n",
    "            artifact_dir = pipeline_config.artifact_dir\n",
    "            experiment_code = pipeline_config.experiment_code\n",
    "            data_validation_config_info = self.config_info.data_validation_config\n",
    "            schema_file_path = os.path.join(CONFIG_DIR, data_validation_config_info.schema_file_name)\n",
    "            data_validated_artifact_dir = Path(\n",
    "                os.path.join(artifact_dir, data_validation_config_info.data_validation_dir, experiment_code))\n",
    "            report_file_dir = os.path.join(data_validated_artifact_dir, data_validation_config_info.report_dir)\n",
    "            create_directories([report_file_dir])\n",
    "\n",
    "            data_validation_config = DataValidationConfig(experiment_code = experiment_code ,\n",
    "                                                          data_validated_artifact_dir= data_validated_artifact_dir,\n",
    "                                                          schema_file_path=schema_file_path,\n",
    "                                                          report_file_dir = report_file_dir , \n",
    "                                                          data_validated_test_collection = data_validation_config_info.data_validated_test_collection_name,\n",
    "                                                          data_validated_train_collection = data_validation_config_info.data_validated_train_collection_name)\n",
    "            return data_validation_config\n",
    "    \n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys)\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        try:\n",
    "            pipeline_config = self.pipeline_config\n",
    "            artifact_dir = pipeline_config.artifact_dir\n",
    "            experiment_code = pipeline_config.experiment_code\n",
    "            random_state  = pipeline_config.training_random_state\n",
    "            data_validation_config_info = self.get_data_validation_config()\n",
    "            data_validated_train_collection = data_validation_config_info.data_validated_train_collection\n",
    "            schema_file_path = data_validation_config_info.schema_file_path\n",
    "            data_transformation_config_info = self.config_info.data_transformation_config\n",
    "            data_transformation_dir_name = data_transformation_config_info.data_transformation_dir\n",
    "            data_transformation_dir = os.path.join(artifact_dir,data_transformation_dir_name, experiment_code)\n",
    "            preprocessed_object_dir = data_transformation_config_info.preprocessing_object_dir\n",
    "            preprocessed_object_name = data_transformation_config_info.preprocessing_object_file_name\n",
    "            preprocessed_object_file_path = os.path.join(data_transformation_dir,preprocessed_object_dir,preprocessed_object_name)\n",
    "            to_train_collection = data_transformation_config_info.to_train_collection\n",
    "            to_test_collection = data_transformation_config_info.to_test_collection\n",
    "\n",
    "            create_directories([os.path.dirname(preprocessed_object_file_path)])\n",
    "            data_transformation_config = DataTransformationConfig(data_validated_train_collection = data_validated_train_collection,\n",
    "                                                                  schema_file_path = schema_file_path ,\n",
    "                                                                  random_state  = random_state,\n",
    "                                                                  preprocessed_object_file_path = preprocessed_object_file_path,\n",
    "                                                                  to_train_collection = to_train_collection,\n",
    "                                                                  to_test_collection = to_test_collection)\n",
    "            return data_transformation_config\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 11:42:47.329 | INFO     | CreditCard.utils.common:read_yaml:34 - yaml file: c:\\Users\\princ\\OneDrive\\Desktop\\project-python\\creditcard\\configs\\config.yaml loaded successfully\n",
      "2023-03-19 11:42:47.331 | INFO     | CreditCard.utils.common:create_directories:53 - created directory at: c:\\Users\\princ\\OneDrive\\Desktop\\project-python\\creditcard\\artifact\n",
      "2023-03-19 11:42:47.333 | INFO     | __main__:get_training_pipeline_config:68 - Training pipeline config: artifact_dir=WindowsPath('c:/Users/princ/OneDrive/Desktop/project-python/creditcard/artifact') pipeline_name='CreditCard' experiment_code='Base_model' training_random_state=1961\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 11:42:48.267 | INFO     | CreditCard.utils.common:create_directories:53 - created directory at: c:\\Users\\princ\\OneDrive\\Desktop\\project-python\\creditcard\\artifact\\stage01_data_validation\\Base_model\\report\n",
      "2023-03-19 11:42:48.270 | INFO     | CreditCard.utils.common:create_directories:53 - created directory at: c:\\Users\\princ\\OneDrive\\Desktop\\project-python\\creditcard\\artifact\\stage02_data_transformation\\Base_model\\preprocessing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataTransformationConfig(data_validated_train_collection='data_validated_train', schema_file_path=WindowsPath('c:/Users/princ/OneDrive/Desktop/project-python/creditcard/configs/schema.yaml'), random_state=1961, preprocessed_object_file_path=WindowsPath('c:/Users/princ/OneDrive/Desktop/project-python/creditcard/artifact/stage02_data_transformation/Base_model/preprocessing/preprocessing_obj.pkl'), to_train_collection='to_train', to_test_collection='to_test')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.get_data_transformation_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creditcard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
