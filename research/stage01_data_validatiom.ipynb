{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\princ\\\\OneDrive\\\\Desktop\\\\project-python\\\\creditcard'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_current_time_stamp():\n",
    "    return f\"{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "\n",
    "ROOT_DIR = os.getcwd()  # to get current working directory\n",
    "CURRENT_TIME_STAMP = get_current_time_stamp()\n",
    "# config constants\n",
    "CONFIG_DIR = os.path.join(ROOT_DIR, \"configs\")\n",
    "CONFIG_FILE_NAME = \"config.yaml\"\n",
    "CONFIG_FILE_PATH = os.path.join(CONFIG_DIR, CONFIG_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel, DirectoryPath, FilePath\n",
    "\n",
    "\n",
    "class DataIngestionConfig(BaseModel):\n",
    "    dataset_download_id: str\n",
    "    raw_data_file_path: Path\n",
    "    ingested_train_file_path: Path\n",
    "    ingested_test_data_path: Path\n",
    "\n",
    "\n",
    "class TrainingPipelineConfig(BaseModel):\n",
    "    artifact_dir: DirectoryPath\n",
    "    pipeline_name: str\n",
    "    experiment_code : str\n",
    "\n",
    "\n",
    "class DataValidationConfig(BaseModel):\n",
    "    experiment_code: str\n",
    "    data_validated_artifact_dir: DirectoryPath\n",
    "    schema_file_path: FilePath\n",
    "    report_file_dir: Path\n",
    "    data_validated_test_collection: str\n",
    "    data_validated_train_collection: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel, DirectoryPath, FilePath\n",
    "\n",
    "\n",
    "class DataIngestionArtifact(BaseModel):\n",
    "    train_file_path: FilePath\n",
    "    test_file_path: FilePath\n",
    "\n",
    "\n",
    "class DataValidationArtifact(BaseModel):\n",
    "    schema_file_path : FilePath\n",
    "    report_file_dir : DirectoryPath\n",
    "    is_validated : bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import (\n",
    "    DataDriftPreset,\n",
    "    DataQualityPreset,\n",
    ")\n",
    "from evidently.metrics import *\n",
    "\n",
    "from evidently.test_suite import TestSuite\n",
    "\n",
    "from evidently.tests import *\n",
    "from box import ConfigBox\n",
    "\n",
    "# from CreditCard.entity import DataValidationConfig\n",
    "# from CreditCard.entity import DataIngestionArtifact, DataValidationArtifact\n",
    "from CreditCard.logging import logger\n",
    "from CreditCard.Database import MongoDB\n",
    "from CreditCard.exception import App_Exception\n",
    "from CreditCard.utils import read_yaml\n",
    "\n",
    "\n",
    "class DataValidation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_validation_config: DataValidationConfig,\n",
    "        data_ingestion_artifact: DataIngestionArtifact,\n",
    "    ):\n",
    "        try:\n",
    "            self.data_validation_config = data_validation_config\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            logger.info(f\"{'>>' * 30}Data Validation log started.{'<<' * 30} \\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise App_Exception(e, sys) from e\n",
    "\n",
    "    def get_train_and_test_df(self):\n",
    "        try:\n",
    "            train_df = pd.read_csv(self.data_ingestion_artifact.train_file_path)\n",
    "            test_df = pd.read_csv(self.data_ingestion_artifact.test_file_path)\n",
    "            train_df.rename(\n",
    "                columns={\"default.payment.next.month\": \"default\"}, inplace=True\n",
    "            )\n",
    "            test_df.rename(\n",
    "                columns={\"default.payment.next.month\": \"default\"}, inplace=True\n",
    "            )\n",
    "            return train_df, test_df\n",
    "        except Exception as e:\n",
    "            raise App_Exception(e, sys) from e\n",
    "\n",
    "    def validate_dataset_schema(\n",
    "        self,\n",
    "    ) -> bool:\n",
    "        try:\n",
    "            logger.info(\"Validating dataset schema\")\n",
    "            validation_status = False\n",
    "            schema_config = read_yaml(\n",
    "                file_path=self.data_validation_config.schema_file_path\n",
    "            )\n",
    "            schema_dict = schema_config.columns\n",
    "            self.train_df, self.test_df = self.get_train_and_test_df()\n",
    "\n",
    "            for column, data_type in schema_dict.items():\n",
    "                self.train_df[column].astype(data_type)\n",
    "                self.test_df[column].astype(data_type)\n",
    "            logger.info(\"Dataset schema validation completed\")\n",
    "            validation_status = True\n",
    "            logger.info(f\"Validation_status {validation_status}\")\n",
    "            return validation_status\n",
    "        except Exception as e:\n",
    "            raise App_Exception(e, sys) from e\n",
    "\n",
    "    def get_current_last_data(self):\n",
    "        current = self.train_df\n",
    "        reference = self.train_connection.find_many_as_df()\n",
    "        return reference, current\n",
    "\n",
    "    def get_and_save_data_drift_report(self):\n",
    "        try:\n",
    "            tests = TestSuite(\n",
    "                tests=[\n",
    "                    TestNumberOfColumnsWithMissingValues(),\n",
    "                    TestNumberOfRowsWithMissingValues(),\n",
    "                    TestNumberOfConstantColumns(),\n",
    "                    TestNumberOfDuplicatedRows(),\n",
    "                    TestNumberOfDuplicatedColumns(),\n",
    "                    TestColumnsType(),\n",
    "                    TestNumberOfDriftedColumns(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            drift_status = True\n",
    "            self.reference, current = self.get_current_last_data()\n",
    "\n",
    "            tests.run(reference_data=self.reference, current_data=current)\n",
    "            TEST_FILE_NAME = (\n",
    "                f\"test_report_{self.data_validation_config.experiment_code}.json\"\n",
    "            )\n",
    "            tests.save_html(\n",
    "                os.path.join(\n",
    "                    self.data_validation_config.report_file_dir, TEST_FILE_NAME\n",
    "                )\n",
    "            )\n",
    "            test_data = ConfigBox(tests.as_dict())\n",
    "            report = Report(metrics=[DataDriftPreset(), DataQualityPreset()])\n",
    "            report.run(reference_data=self.reference, current_data=current)\n",
    "            PROFILE_FILE_NAME = TEST_FILE_NAME.replace(\"json\", \"html\")\n",
    "            report.save_html(\n",
    "                os.path.join(\n",
    "                    self.data_validation_config.report_file_dir, PROFILE_FILE_NAME\n",
    "                )\n",
    "            )\n",
    "            report_data = ConfigBox(tests.as_dict())\n",
    "            if not (\n",
    "                report_data.metrics[0].result.dataset_drift\n",
    "                and test_data.summary.all_passed\n",
    "            ):\n",
    "                drift_status = False\n",
    "            return drift_status\n",
    "\n",
    "        except Exception as e:\n",
    "            raise App_Exception(e, sys) from e\n",
    "        \n",
    "    def check_data_to_insert( self,new_data : pd.DataFrame, reference_data : pd.DataFrame):\n",
    "        data_insert_status = reference_data.equals(new_data)\n",
    "        if not data_insert_status:\n",
    "            index_to_insert = [ele for ele in range(len(new_data)) if new_data[ele , 0] not in reference_data.ID.to_list]\n",
    "            data_to_insert = new_data.loc[index_to_insert]\n",
    "        return data_insert_status , data_to_insert\n",
    "            \n",
    "        \n",
    "\n",
    "    def initiate_data_validation(self) -> DataValidationArtifact:\n",
    "        try:\n",
    "            data_validation_config_info = self.data_validation_config\n",
    "            test_collections = (\n",
    "                data_validation_config_info.data_validated_test_collection\n",
    "            )\n",
    "            train_collections = (\n",
    "                data_validation_config_info.data_validated_train_collection\n",
    "            )\n",
    "            self.test_connection = MongoDB(test_collections, drop_collection=False)\n",
    "            self.train_connection = MongoDB(train_collections, drop_collection=False)\n",
    "\n",
    "            validation_status = self.validate_dataset_schema()\n",
    "            data_drift = self.get_and_save_data_drift_report()\n",
    "            if data_drift:\n",
    "                raise Exception(\"Data drift found\")\n",
    "            if validation_status and not (data_drift):\n",
    "                logger.info(\"Data validation completed\")\n",
    "                train_insert_status, train_to_insert = self.check_data_to_insert(\n",
    "                    self.train_df, self.reference\n",
    "                )\n",
    "                test_reference = self.test_connection.find_many_as_df()\n",
    "                test_insert_status, test_to_insert = self.check_data_to_insert(\n",
    "                    self.test_df, test_reference\n",
    "                )\n",
    "                if train_insert_status:\n",
    "                    self.train_connection.Insert_Many(\n",
    "                        train_to_insert.to_dict(orient=\"records\")\n",
    "                    )\n",
    "                if test_insert_status:\n",
    "                    self.test_connection.Insert_Many(\n",
    "                        test_to_insert.to_dict(orient=\"records\")\n",
    "                    )\n",
    "                validation_status = True\n",
    "                logger.info(\"Data Validation successfully.\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                validation_status = False\n",
    "                logger.info(\"Data Validation not successfully.\")\n",
    "                \n",
    "            data_validation_artifact = DataValidationArtifact(\n",
    "                    schema_file_path=self.data_validation_config.schema_file_path,\n",
    "                    report_file_dir=self.data_validation_config.report_file_dir,\n",
    "                    is_validated=validation_status,)\n",
    "\n",
    "            return data_validation_artifact\n",
    "\n",
    "        except Exception as e:\n",
    "            raise App_Exception(e, sys) from e\n",
    "\n",
    "    def __del__(self):\n",
    "        logger.info(f\"{'>>' * 30}Data Validation log completed.{'<<' * 30} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import  os\n",
    "import  json\n",
    "\n",
    "# from CreditCard.entity import DataIngestionConfig ,  TrainingPipelineConfig\n",
    "from CreditCard.exception import App_Exception\n",
    "from CreditCard.logging import logger\n",
    "from CreditCard.utils import read_yaml , create_directories\n",
    "from pathlib import Path\n",
    "from CreditCard.constants import CONFIG_FILE_PATH ,  CURRENT_TIME_STAMP , ROOT_DIR , CONFIG_DIR\n",
    "\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "\n",
    "    def __init__(self,\n",
    "                 config_file_path: Path = CONFIG_FILE_PATH) -> None:\n",
    "        try:\n",
    "            self.config_info = read_yaml(path_to_yaml=Path(config_file_path))\n",
    "            self.pipeline_config = self.get_training_pipeline_config()\n",
    "            self.time_stamp = CURRENT_TIME_STAMP\n",
    "\n",
    "        except Exception as e:\n",
    "            raise App_Exception(e, sys) from e\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \n",
    "        try:\n",
    "            data_ingestion_info = self.config_info.data_ingestion_config\n",
    "            artifact_dir = self.pipeline_config.artifact_dir\n",
    "            dataset_download_id = data_ingestion_info.dataset_download_id\n",
    "            data_ingestion_dir_name = data_ingestion_info.ingestion_dir\n",
    "            raw_data_dir = data_ingestion_info.raw_data_dir\n",
    "            raw_file_name = data_ingestion_info.dataset_download_file_name\n",
    "            data_ingestion_dir = os.path.join(artifact_dir, data_ingestion_dir_name)\n",
    "            raw_data_file_path  = os.path.join(data_ingestion_dir, raw_data_dir, raw_file_name)\n",
    "            ingested_dir_name = data_ingestion_info.ingested_dir\n",
    "            ingested_dir_path = os.path.join(data_ingestion_dir,ingested_dir_name)\n",
    "            \n",
    "            ingested_train_file_path  = os.path.join(ingested_dir_path, data_ingestion_info.ingested_train_file)\n",
    "            ingested_test_file_path = os.path.join(ingested_dir_path, data_ingestion_info.ingested_test_file)\n",
    "            create_directories([os.path.dirname(raw_data_file_path), os.path.dirname(ingested_train_file_path)])\n",
    "            \n",
    "            data_ingestion_config = DataIngestionConfig(dataset_download_id = dataset_download_id , \n",
    "                                                        raw_data_file_path = raw_data_file_path , \n",
    "                                                        ingested_train_file_path = ingested_train_file_path , \n",
    "                                                        ingested_test_data_path  = ingested_test_file_path)\n",
    "            \n",
    "            return data_ingestion_config\n",
    "        except Exception as e:\n",
    "            raise App_Exception(e, sys) from e\n",
    "    def get_training_pipeline_config(self) -> TrainingPipelineConfig:\n",
    "        try:\n",
    "            training_config = self.config_info.training_pipeline_config\n",
    "            training_pipeline_name = training_config.pipeline_name\n",
    "            training_experiment_code = training_config.experiment_code\n",
    "            training_artifacts = os.path.join(ROOT_DIR, training_config.artifact_dir , training_experiment_code)\n",
    "            create_directories(path_to_directories = [training_artifacts])\n",
    "            training_pipeline_config =  TrainingPipelineConfig(artifact_dir=training_artifacts ,\n",
    "                                                               experiment_code = training_experiment_code,\n",
    "                                                               pipeline_name=training_pipeline_name)\n",
    "            logger.info(f\"Training pipeline config: {training_pipeline_config}\")\n",
    "            return training_pipeline_config\n",
    "        except Exception as e:\n",
    "            raise App_Exception(e, sys) from e\n",
    "        \n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        try:\n",
    "            artifact_dir = self.pipeline_config.artifact_dir\n",
    "            experiment_code = self.pipeline_config.experiment_code \n",
    "            data_validation_config_info = self.config_info.data_validation_config\n",
    "            data_validated_artifact_dir = Path(os.path.join(artifact_dir,data_validation_config_info.data_validation_dir))\n",
    "            schema_file_path = os.path.join(CONFIG_DIR, data_validation_config_info.schema_file_name)\n",
    "            report_file_dir = os.path.join(data_validated_artifact_dir, data_validation_config_info.report_dir)\n",
    "            \n",
    "            create_directories([report_file_dir])\n",
    "\n",
    "            data_validation_config = DataValidationConfig(experiment_code = experiment_code ,\n",
    "                                                          data_validated_artifact_dir= data_validated_artifact_dir,\n",
    "                                                          schema_file_path=schema_file_path,\n",
    "                                                          report_file_dir = report_file_dir , \n",
    "                                                          data_validated_test_collection = data_validation_config_info.data_validated_test_collection_name,\n",
    "                                                          data_validated_train_collection = data_validation_config_info.data_validated_train_collection_name)\n",
    "            return data_validation_config\n",
    "        except Exception as e:\n",
    "            raise App_Exception(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 12:52:53.406 | INFO     | CreditCard.utils.common:read_yaml:34 - yaml file: c:\\Users\\princ\\OneDrive\\Desktop\\project-python\\creditcard\\configs\\config.yaml loaded successfully\n",
      "2023-03-17 12:52:53.408 | INFO     | CreditCard.utils.common:create_directories:53 - created directory at: c:\\Users\\princ\\OneDrive\\Desktop\\project-python\\creditcard\\artifact\\Base_model\n",
      "2023-03-17 12:52:53.410 | INFO     | __main__:get_training_pipeline_config:62 - Training pipeline config: artifact_dir=WindowsPath('c:/Users/princ/OneDrive/Desktop/project-python/creditcard/artifact/Base_model') pipeline_name='CreditCard' experiment_code='Base_model'\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 12:52:53.805 | INFO     | CreditCard.utils.common:create_directories:53 - created directory at: c:\\Users\\princ\\OneDrive\\Desktop\\project-python\\creditcard\\artifact\\Base_model\\stage01_data_validation\\report\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataValidationConfig(experiment_code='Base_model', data_validated_artifact_dir=WindowsPath('c:/Users/princ/OneDrive/Desktop/project-python/creditcard/artifact/Base_model/stage01_data_validation'), schema_file_path=WindowsPath('c:/Users/princ/OneDrive/Desktop/project-python/creditcard/configs/schema.yaml'), report_file_dir=WindowsPath('c:/Users/princ/OneDrive/Desktop/project-python/creditcard/artifact/Base_model/stage01_data_validation/report'), data_validated_test_collection='data_validated_test', data_validated_train_collection='data_validated_train')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.get_data_validation_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creditcard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
