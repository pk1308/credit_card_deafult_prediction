{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_current_time_stamp():\n",
    "    return f\"{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "\n",
    "ROOT_DIR = os.getcwd()  # to get current working directory\n",
    "CURRENT_TIME_STAMP = get_current_time_stamp()\n",
    "# config constants\n",
    "CONFIG_DIR = os.path.join(ROOT_DIR, 'configs')\n",
    "CONFIG_FILE_NAME = \"config.yaml\"\n",
    "CONFIG_FILE_PATH = os.path.join(CONFIG_DIR, CONFIG_FILE_NAME)\n",
    "\n",
    "DATABASE_FILE_NAME = \"database.yaml\"\n",
    "DATABASE_FILE = Path(os.path.join(CONFIG_DIR, DATABASE_FILE_NAME))\n",
    "\n",
    "FEATURE_GENERATOR_FILE_NAME = \"feature_generator.yaml\"\n",
    "FEATURE_GENERATOR_FILE_PATH = Path(os.path.join(CONFIG_DIR, FEATURE_GENERATOR_FILE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, DirectoryPath, FilePath\n",
    "\n",
    "\n",
    "class DataIngestionArtifact(BaseModel):\n",
    "    train_file_path: FilePath\n",
    "    test_file_path: FilePath\n",
    "\n",
    "\n",
    "class DataValidationArtifact(BaseModel):\n",
    "    schema_file_path: FilePath\n",
    "    report_file_dir: DirectoryPath\n",
    "    is_validated: bool\n",
    "\n",
    "\n",
    "class DataTransformationArtifact(BaseModel):\n",
    "    preprocessed_object_path: FilePath\n",
    "    \n",
    "class ModelTrainerArtifact(BaseModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel, DirectoryPath, FilePath\n",
    "\n",
    "\n",
    "class DataIngestionConfig(BaseModel):\n",
    "    dataset_download_id: str\n",
    "    raw_data_file_path: Path\n",
    "    ingested_train_file_path: Path\n",
    "    ingested_test_data_path: Path\n",
    "    random_state: int\n",
    "\n",
    "\n",
    "class TrainingPipelineConfig(BaseModel):\n",
    "    artifact_dir: DirectoryPath\n",
    "    pipeline_name: str\n",
    "    experiment_code: str\n",
    "    training_random_state: int\n",
    "\n",
    "\n",
    "class DataValidationConfig(BaseModel):\n",
    "    experiment_code: str\n",
    "    data_validated_artifact_dir: DirectoryPath\n",
    "    schema_file_path: FilePath\n",
    "    report_file_dir: Path\n",
    "    data_validated_test_collection: str\n",
    "    data_validated_train_collection: str\n",
    "    train_data_file: FilePath\n",
    "    test_data_file: FilePath\n",
    "\n",
    "\n",
    "class DataTransformationConfig(BaseModel):\n",
    "    data_validated_train_collection: str\n",
    "    schema_file_path: FilePath\n",
    "    random_state: int\n",
    "    preprocessed_object_file_path: Path\n",
    "    to_train_collection: str\n",
    "    to_test_collection: str\n",
    "    \n",
    "class ModelTrainerConfig(BaseModel):\n",
    "    model_config_file_path : Path\n",
    "    base_accuracy : int\n",
    "    trained_model_file_path : Path \n",
    "    model_report_dir : DirectoryPath\n",
    "    preprocessed_object_file_path: FilePath\n",
    "    to_train_collection: str\n",
    "    to_test_collection: str\n",
    "    schema_file_path : FilePath\n",
    "    model_report_dir : DirectoryPath\n",
    "    eval_difference : int\n",
    "    eval_param  : str\n",
    "    experiment_id : str\n",
    "    validated_collection : str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from CreditCard.constants import (CONFIG_DIR, CONFIG_FILE_PATH,\n",
    "                                  CURRENT_TIME_STAMP, ROOT_DIR)\n",
    "from CreditCard.entity import (DataIngestionConfig, DataTransformationConfig,\n",
    "                               DataValidationConfig, TrainingPipelineConfig)\n",
    "from CreditCard.exception import AppException\n",
    "from CreditCard.logging import logger\n",
    "from CreditCard.utils import create_directories, read_yaml\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "\n",
    "    def __init__(self,\n",
    "                 config_file_path: Path = CONFIG_FILE_PATH) -> None:\n",
    "        try:\n",
    "            self.config_info = read_yaml(path_to_yaml=Path(config_file_path))\n",
    "            self.pipeline_config = self.get_training_pipeline_config()\n",
    "            self.time_stamp = CURRENT_TIME_STAMP\n",
    "\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "\n",
    "        try:\n",
    "            data_ingestion_info = self.config_info.data_ingestion_config\n",
    "            pipeline_config = self.pipeline_config\n",
    "            artifact_dir = pipeline_config.artifact_dir\n",
    "            experiment_code = pipeline_config.experiment_code\n",
    "            random_state = pipeline_config.training_random_state\n",
    "            dataset_download_id = data_ingestion_info.dataset_download_id\n",
    "            data_ingestion_dir_name = data_ingestion_info.ingestion_dir\n",
    "            raw_data_dir = data_ingestion_info.raw_data_dir\n",
    "            raw_file_name = data_ingestion_info.dataset_download_file_name\n",
    "            data_ingestion_dir = os.path.join(artifact_dir, data_ingestion_dir_name, experiment_code)\n",
    "            raw_data_file_path = os.path.join(data_ingestion_dir, raw_data_dir, raw_file_name)\n",
    "            ingested_dir_name = data_ingestion_info.ingested_dir\n",
    "            ingested_dir_path = os.path.join(data_ingestion_dir, ingested_dir_name)\n",
    "\n",
    "            ingested_train_file_path = os.path.join(ingested_dir_path, data_ingestion_info.ingested_train_file)\n",
    "            ingested_test_file_path = os.path.join(ingested_dir_path, data_ingestion_info.ingested_test_file)\n",
    "            create_directories([os.path.dirname(raw_data_file_path), os.path.dirname(ingested_train_file_path)])\n",
    "\n",
    "            data_ingestion_config = DataIngestionConfig(dataset_download_id=dataset_download_id,\n",
    "                                                        raw_data_file_path=raw_data_file_path,\n",
    "                                                        ingested_train_file_path=ingested_train_file_path,\n",
    "                                                        ingested_test_data_path=ingested_test_file_path,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "            return data_ingestion_config\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "\n",
    "    def get_training_pipeline_config(self) -> TrainingPipelineConfig:\n",
    "        try:\n",
    "            training_config = self.config_info.training_pipeline_config\n",
    "            training_pipeline_name = training_config.pipeline_name\n",
    "            training_experiment_code = training_config.experiment_code\n",
    "            training_random_state = training_config.random_state\n",
    "            training_artifacts = os.path.join(ROOT_DIR, training_config.artifact_dir)\n",
    "            create_directories(path_to_directories=[training_artifacts])\n",
    "            training_pipeline_config = TrainingPipelineConfig(artifact_dir=training_artifacts,\n",
    "                                                              experiment_code=training_experiment_code,\n",
    "                                                              pipeline_name=training_pipeline_name,\n",
    "                                                              training_random_state=training_random_state)\n",
    "            logger.info(f\"Training pipeline config: {training_pipeline_config}\")\n",
    "            return training_pipeline_config\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        try:\n",
    "            pipeline_config = self.pipeline_config\n",
    "            artifact_dir = pipeline_config.artifact_dir\n",
    "            data_ingestion_config = self.get_data_ingestion_config()\n",
    "            train_data_file = data_ingestion_config.ingested_train_file_path\n",
    "            test_data_file = data_ingestion_config.ingested_test_data_path\n",
    "            experiment_code = pipeline_config.experiment_code\n",
    "            data_validation_config_info = self.config_info.data_validation_config\n",
    "            data_validated_artifact_dir = Path(\n",
    "                os.path.join(artifact_dir, data_validation_config_info.data_validation_dir, experiment_code))\n",
    "            schema_file_path = os.path.join(CONFIG_DIR, data_validation_config_info.schema_file_name)\n",
    "            report_file_dir = os.path.join(data_validated_artifact_dir, data_validation_config_info.report_dir)\n",
    "            validated_test_collection = data_validation_config_info.data_validated_test_collection_name\n",
    "            validated_train_collection = data_validation_config_info.data_validated_train_collection_name\n",
    "            create_directories([report_file_dir])\n",
    "\n",
    "            data_validation_config = DataValidationConfig(experiment_code=experiment_code,\n",
    "                                                          data_validated_artifact_dir=data_validated_artifact_dir,\n",
    "                                                          schema_file_path=schema_file_path,\n",
    "                                                          report_file_dir=report_file_dir,\n",
    "                                                          data_validated_test_collection=validated_test_collection,\n",
    "                                                          data_validated_train_collection=validated_train_collection,\n",
    "                                                          train_data_file=train_data_file,\n",
    "                                                          test_data_file=test_data_file)\n",
    "            return data_validation_config\n",
    "\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys)\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        try:\n",
    "            pipeline_config = self.pipeline_config\n",
    "            artifact_dir = pipeline_config.artifact_dir\n",
    "            experiment_code = pipeline_config.experiment_code\n",
    "            random_state = pipeline_config.training_random_state\n",
    "            data_validation_config_info = self.get_data_validation_config()\n",
    "            data_validated_train_collection = data_validation_config_info.data_validated_train_collection\n",
    "            schema_file_path = data_validation_config_info.schema_file_path\n",
    "            data_transformation_config_info = self.config_info.data_transformation_config\n",
    "            data_transformation_dir_name = data_transformation_config_info.data_transformation_dir\n",
    "            data_transformation_dir = os.path.join(artifact_dir, data_transformation_dir_name, experiment_code)\n",
    "            preprocessed_object_dir = data_transformation_config_info.preprocessing_object_dir\n",
    "            preprocessed_object_name = data_transformation_config_info.preprocessing_object_file_name\n",
    "            preprocessed_object_file_path = os.path.join(data_transformation_dir, preprocessed_object_dir,\n",
    "                                                         preprocessed_object_name)\n",
    "            to_train_collection = data_transformation_config_info.to_train_collection\n",
    "            to_test_collection = data_transformation_config_info.to_test_collection\n",
    "\n",
    "            create_directories([os.path.dirname(preprocessed_object_file_path)])\n",
    "            data_transformation_config = DataTransformationConfig(\n",
    "                data_validated_train_collection=data_validated_train_collection,\n",
    "                schema_file_path=schema_file_path,\n",
    "                random_state=random_state,\n",
    "                preprocessed_object_file_path=preprocessed_object_file_path,\n",
    "                to_train_collection=to_train_collection,\n",
    "                to_test_collection=to_test_collection)\n",
    "            return data_transformation_config\n",
    "\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CreditCard.exception import AppException\n",
    "from CreditCard.logging import logger\n",
    "# from CreditCard.entity import *\n",
    "import pandas as pd \n",
    "from CreditCard.utils import save_bin , load_bin\n",
    "from CreditCard.Database import MongoDB\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BaseModel:\n",
    "    \"\"\"model estimator : Train the model and save the model to pickle \"\"\"\n",
    "    def __init__(self, preprocessing_object, trained_model_object):\n",
    "        \"\"\"\n",
    "        TrainedModel constructor\n",
    "        preprocessing_object: preprocessing_object\n",
    "        trained_model_dict:  {cluster : model saved path}\n",
    "        \"\"\"\n",
    "        self.preprocessing_object = preprocessing_object\n",
    "        self.trained_model_object = trained_model_object\n",
    "        self.columns_to_drop = [\"cluster\"]\n",
    "        \n",
    "    def preprocess_data(self,data_to_preprocess):\n",
    "        transformed_feature_to_predict = self.preprocessing_object.transform_data(data_to_preprocess)\n",
    "        transformed_feature_to_predict.drop(self.columns_to_drop , axis =1 , inplace = True)\n",
    "        return transformed_feature_to_predict\n",
    "       \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        function accepts raw inputs and then transformed raw input using preprocessing_object\n",
    "        which guarantees that the inputs are in the same format as the training data\n",
    "        At last it perform prediction on transformed features\n",
    "        \"\"\"\n",
    "        data_to_predict = self.preprocess_data(data_to_preprocess=X)\n",
    "        prediction = self.trained_model_object.predict(data_to_predict)\n",
    "        return prediction\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        data_to_predict = self.preprocess_data(data_to_preprocess=X)\n",
    "        prediction_proba = self.trained_model_object.predict_proba(data_to_predict)\n",
    "        return prediction_proba\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{type(self.trained_model_object).__name__}()\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{type(self.trained_model_object).__name__}()\"\n",
    "\n",
    "class EstimatorModel:\n",
    "    \"\"\"model estimator : Train the model and save the model to pickle \"\"\"\n",
    "    def __init__(self, preprocessing_object, trained_model_dict):\n",
    "        \"\"\"\n",
    "        TrainedModel constructor\n",
    "        preprocessing_object: preprocessing_object\n",
    "        trained_model_dict:  {cluster : model saved path}\n",
    "        \"\"\"\n",
    "        self.preprocessing_object = preprocessing_object\n",
    "        self.trained_model_dict = trained_model_dict\n",
    "        self.trained_model_object = {cluster : load_bin(path = model) for cluster , model in trained_model_dict.items()}\n",
    "        \n",
    "    def preprocess_data(self,data_to_preprocess):\n",
    "        transformed_feature_to_predict = self.preprocessing_object.transform(data_to_preprocess)\n",
    "        return transformed_feature_to_predict\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        function accepts raw inputs and then transformed raw input using preprocessing_object\n",
    "        which guarantees that the inputs are in the same format as the training data\n",
    "        At last it perform prediction on transformed features\n",
    "        \"\"\"\n",
    "        pre_data_to_predict = self.preprocess_data(data_to_preprocess=X)\n",
    "        prediction = pd.DataFrame(columns=[\"prediction\"])\n",
    "        \n",
    "        for row in range(pre_data_to_predict.shape[0]):\n",
    "            data_to_predict = pre_data_to_predict.loc[row]\n",
    "            cluster= data_to_predict[\"cluster\"]\n",
    "            model = self.trained_model_object[cluster]\n",
    "            prediction.loc[row] = model.predict(data_to_predict.drop(labels=[\"cluster\"] , axis = 1))\n",
    "\n",
    "        return prediction\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        function accepts raw inputs and then transformed raw input using preprocessing_object\n",
    "        which guarantees that the inputs are in the same format as the training data\n",
    "        At last it perform prediction on transformed features\n",
    "        \"\"\"\n",
    "        pre_data_to_predict = self.preprocess_data(data_to_preprocess=X)\n",
    "        prediction = pd.DataFrame(columns=[\"0\" , \"1\"])\n",
    "        \n",
    "        for row in range(pre_data_to_predict.shape[0]):\n",
    "            data_to_predict = pre_data_to_predict.loc[row]\n",
    "            cluster= data_to_predict[\"cluster\"]\n",
    "            model = self.trained_model_object[cluster]\n",
    "            prediction.loc[row] = model.predict_proba(data_to_predict.drop(labels=[\"cluster\"] , axis = 1))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{type(self.trained_model_object).__name__}()\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{type(self.trained_model_object).__name__}()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel, DirectoryPath, FilePath\n",
    "\n",
    "class MetricReportArtifact(BaseModel):\n",
    "        experiment_id : str \n",
    "        model_name : str \n",
    "        model_obj : object\n",
    "        report : object\n",
    "\n",
    "class MetricEvalArtifact(BaseModel):\n",
    "        best_model : object\n",
    "        best_train_eval_param  : int\n",
    "        best_test_eval_param : int\n",
    "        best_eval_param_difference : int \n",
    "        best_model_name  : str\n",
    "        best_model_report : object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from CreditCard.logging import logger\n",
    "from CreditCard.exception import AppException\n",
    "from ensure import ensure_annotations\n",
    "from evidently.metrics import ClassificationQualityMetric\n",
    "from evidently.metrics import ClassificationClassBalance\n",
    "from evidently.metrics import ClassificationConfusionMatrix\n",
    "from evidently.metrics import ClassificationQualityByClass\n",
    "from evidently.metrics import ClassificationClassSeparationPlot\n",
    "from evidently.metrics import ClassificationProbDistribution\n",
    "from evidently.metrics import ClassificationRocCurve\n",
    "from evidently.metrics import ClassificationPRCurve\n",
    "from evidently.metrics import ClassificationPRTable\n",
    "from evidently.metrics import ClassificationQualityByFeatureTable\n",
    "from evidently.metrics import ConflictTargetMetric\n",
    "from evidently.metrics import ConflictPredictionMetric\n",
    "from evidently.report import Report\n",
    "from box import ConfigBox\n",
    "\n",
    "@ensure_annotations\n",
    "def  get_best_model(reports_artifacts: list , eval_param : str , eval_difference : float , base_accuracy : float):\n",
    "        best_model = None\n",
    "        best_train_eval_param = None \n",
    "        best_test_eval_param = None\n",
    "        best_eval_param_difference = None \n",
    "        best_model_name = None \n",
    "        best_model_report = eval_difference\n",
    "        for report_artifact in reports_artifacts:\n",
    "            classification_report = report_artifact.report\n",
    "            data = ConfigBox(classification_report.as_dict())\n",
    "            test_data_result = data.metrics[0].result.current\n",
    "            train_data_result = data.metrics[0].result.reference\n",
    "            test_eval_param = test_data_result[eval_param]\n",
    "            train_eval_param = train_data_result[eval_param]\n",
    "            model_eval_difference = train_eval_param - test_eval_param\n",
    "            if model_eval_difference < best_eval_param_difference and test_data_result[\"accuracy\"] >= base_accuracy :\n",
    "                best_model= report_artifact.model_obj\n",
    "                best_train_eval_param = train_eval_param\n",
    "                best_test_eval_param = test_eval_param\n",
    "                best_eval_param_difference = model_eval_difference\n",
    "                best_model_name = report_artifact.model_name\n",
    "                best_model_report = report_artifact.report\n",
    "        if best_model :\n",
    "            \n",
    "            model_eval_artifact = MetricEvalArtifact(best_model = best_model,\n",
    "                                                     best_train_eval_param= best_train_eval_param,\n",
    "                                                     best_test_eval_param= best_test_eval_param ,\n",
    "                                                     best_eval_param_difference= best_eval_param_difference , \n",
    "                                                     best_model_name= best_model_name  ,\n",
    "                                                     best_model_report= best_model_report )\n",
    "            return model_eval_artifact\n",
    "\n",
    "\n",
    "@ensure_annotations\n",
    "def evaluate_classification_model(X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "                                  X_test: pd.DataFrame, y_test: pd.DataFrame, base_accuracy : float  ,report_dir : str,\n",
    "                                  eval_difference : float , estimators: list,  eval_param : str = \"accuracy\" ,\n",
    "                                  experiment_id: str = None ) -> MetricEvalArtifact:\n",
    "    \"\"\"\n",
    "      Description:\n",
    "      This function compare multiple regression model return best model\n",
    "      Params:\n",
    "      experiment_id: the experiment id\n",
    "      estimators: model List \n",
    "      X_train: Training dataset input feature\n",
    "      y_train: Training dataset target feature\n",
    "      X_test: Testing dataset input feature\n",
    "      y_test: Testing dataset input feature\n",
    "      return\n",
    "      It returned a named tuple\n",
    "\n",
    "      MetricInfoArtifact (\"model_name\", \"model_object\",\"train_precision\", \"test_precision\",\n",
    "                                                            \"train_recall\", \"test_recall\",\n",
    "                                                            \"train_f1\" , \"test_f1\",\"model_accuracy\", \"index_number\")\n",
    "    \"\"\"\n",
    "    current_df = X_test.copy()\n",
    "    current_df[\"target\"] = y_test.copy()\n",
    "    reference_df = X_train.copy()\n",
    "    reference_df[\"target\"] = y_train.copy()\n",
    "    \n",
    "    if experiment_id is None:\n",
    "        experiment_id = f\"{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "        \n",
    "    model_dir = os.path.join(report_dir, \"model_report\")\n",
    "    experiment_dir = os.path.join(model_dir, experiment_id)\n",
    "    model_report_list = list()\n",
    "    try:\n",
    "        best_model = None\n",
    "        for estimator in estimators:\n",
    "            model_name = estimator.__class__.__name__\n",
    "            model = estimator\n",
    "            current_df['prediction'] = model.predict_proba(current_df[X_test.feature_names.tolist()]).iloc[:, 1]\n",
    "            reference_df['prediction'] = model.predict_proba(reference_df[X_train.feature_names.tolist()]).iloc[:, 1]\n",
    "            classification_report = Report(metrics=[ClassificationQualityMetric(),\n",
    "                                                     ClassificationClassBalance(),\n",
    "                                                     ConflictTargetMetric(),\n",
    "                                                     ConflictPredictionMetric(),\n",
    "                                                    ClassificationConfusionMatrix(),\n",
    "                                                    ClassificationQualityByClass(),\n",
    "                                                    ClassificationClassSeparationPlot(),\n",
    "                                                    ClassificationProbDistribution(),\n",
    "                                                    ClassificationRocCurve(),\n",
    "                                                    ClassificationPRCurve(),\n",
    "                                                    ClassificationPRTable(),\n",
    "                                                    ClassificationQualityByFeatureTable(),])\n",
    "            model_report_path = os.path_join(experiment_dir, model_name, \"evidently_classification_report.html\")\n",
    "            classification_report.save_html(filename=model_report_path)\n",
    "            model_report_artifact = MetricReportArtifact(experiment_id = experiment_id , \n",
    "                                                       model_name = model_name , \n",
    "                                                       model_obj = model , \n",
    "                                                       report  = classification_report)\n",
    "            model_report_list.append(model_report_artifact)\n",
    "        best_model = get_best_model(reports = model_report_list , eval_param = eval_param  ,eval_difference =eval_difference , \n",
    "                                    base_accuracy =base_accuracy )\n",
    "\n",
    "            \n",
    "        if best_model is None:\n",
    "            logger.info(\"No acceptable model found\")\n",
    "        else:\n",
    "            logger.info(f\"Acceptable model  name {best_model.best_model}. \")\n",
    "            logger.info(f\"Acceptable model test {eval_param} score {best_model.best_test_eval_param}. \")\n",
    "            logger.info(f\"best model artifact {best_model}\")\n",
    "    except Exception as e:\n",
    "        raise AppException(e, sys)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import yaml\n",
    "from CreditCard.exception import AppException \n",
    "import os\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "from typing import List\n",
    "from CreditCard.logging import logger\n",
    "from CreditCard.utils import read_yaml\n",
    "from neuro_mf import ModelFactory , GridSearchedBestModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "class ModelTrainer:\n",
    "\n",
    "    def __init__(self, model_trainer_config: ModelTrainerConfig):\n",
    "        try:\n",
    "            logger.info(f\"{'>>' * 30}Model trainer log started.{'<<' * 30} \")\n",
    "            self.model_trainer_config_info = model_trainer_config\n",
    "            to_train_collection = self.model_trainer_config_info.to_train_collection\n",
    "            to_test_collection = self.model_trainer_config_info.to_test_collection\n",
    "            validated_collection = self.model_trainer_config_info.validated_collection\n",
    "            to_train_connection = MongoDB(to_train_collection, drop_collection=False)\n",
    "            to_test_connection = MongoDB(to_test_collection, drop_collection=False)\n",
    "            validated_connection = MongoDB(validated_collection , drop_collection=False)\n",
    "            self.validated_df = validated_connection.find_many_as_df()\n",
    "            self.train_df = to_train_connection.find_many_as_df()\n",
    "            self.test_df = to_test_connection.find_many_as_df()\n",
    "            preprocessing_obj_path = self.model_trainer_config_info.preprocessed_object_file_path\n",
    "            self.preprocessing_obj = load_bin(path=preprocessing_obj_path)\n",
    "            self.schema = read_yaml(path_to_yaml=self.model_trainer_config_info.schema_file_path)\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "    \n",
    "    def get_validated_data_to_test(self):\n",
    "        data = self.validated_df.copy()\n",
    "        master_x = data[self.schema.columns_to_test]\n",
    "        master_y = data[self.schema.target_test]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(master_x, master_y, test_size=0.2, random_state=1961)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def initiate_model_trainer(self) -> ModelTrainerArtifact:\n",
    "        try:\n",
    "            logger.info(\"Loading transformed training dataset\")\n",
    "            logger.info(f\"{'>>' * 30}Base Model.{'<<' * 30} \")\n",
    "            model_trainer_config_info = self.model_trainer_config_info\n",
    "            trained_model_file_path = model_trainer_config_info.trained_model_file_path\n",
    "            model_report_dir = model_trainer_config_info.model_report_dir\n",
    "            eval_difference = model_trainer_config_info.eval_difference\n",
    "            base_accuracy = model_trainer_config_info.base_accuracy\n",
    "            model_config_file_path = model_trainer_config_info.model_config_file_path\n",
    "            eval_difference = model_trainer_config_info.eval_difference\n",
    "            eval_param = model_trainer_config_info.eval_param\n",
    "            experiment_id = model_trainer_config_info.experiment_id\n",
    "            base_features_to_drop = self.schema.base_model_features_to_drop\n",
    "            base_x_train = self.train_df.drop(base_features_to_drop, axis =1 )\n",
    "            base_y_train = self.train_df[self.schema.target_column]\n",
    "            base_x_test = self.test_df.drop(base_features_to_drop, axis =1 )\n",
    "            base_y_test = self.test_df[self.schema.target_column]\n",
    "            logger.info(f\"Expected accuracy: {base_accuracy}\")\n",
    "            logger.info(\"Extracting model config file path\")\n",
    "            base_model_factory = ModelFactory(model_config_path=model_config_file_path)\n",
    "            base_best_model =base_model_factory.get_best_model(X=base_x_train, y=base_y_train, base_accuracy=base_accuracy)\n",
    "            base_grid_searched_best_model_list: List[GridSearchedBestModel] = base_model_factory.grid_searched_best_model_list\n",
    "            base_model_list = [model.best_model for model in base_grid_searched_best_model_list]\n",
    "            base_report_dir = os.path.join(model_report_dir, \"Base_model\")\n",
    "            \n",
    "            base_metric_info : MetricEvalArtifact = evaluate_classification_model(estimators=base_model_list, X_train=base_x_train,\n",
    "                                                                            y_train=base_y_train, X_test=base_x_test, y_test=base_y_test,\n",
    "                                                                            base_accuracy=base_accuracy , report_dir=base_report_dir,\n",
    "                                                                            eval_difference =eval_difference  , eval_param = eval_param ,\n",
    "                                                                            experiment_id= experiment_id )\n",
    "            \n",
    "    \n",
    "            logger.info(f\"{base_metric_info.__dict__}\")\n",
    "            base_model_file_name = f\"{base_metric_info.best_model_name}.pkl\"\n",
    "            base_model_file_path = os.path.join(base_report_dir, \"Model\", base_model_file_name)\n",
    "            base_model_object = base_metric_info.best_model\n",
    "            base_predictor = BaseModel(preprocessing_object=self.preprocessing_obj , trained_model_object=base_model_object)\n",
    "            save_bin(data = base_predictor, path = base_model_file_path)\n",
    "            \n",
    "            logger.info(f\"{'>>' * 30}Base model done{'<<' * 30} \")\n",
    "            cluster_model_dict = dict()\n",
    "            clusters = self.train_df[\"cluster\"].unique()\n",
    "            for cluster in clusters:\n",
    "                model_list = None\n",
    "                to_train_df = self.train_df[self.train_df[self.schema.cluster_column]==cluster]\n",
    "                to_test_df = self.test_df[self.test_df[self.schema.cluster_column]==cluster]\n",
    "                to_train_features = to_train_df.drop(self.schema.to_drop_cluster_data , axis=1)\n",
    "                to_train_target = to_test_df[self.schema.target_column]\n",
    "                to_test_features = to_test_df.drop(self.schema.to_drop_cluster_data , axis =1 )\n",
    "                to_test_target = to_test_df[self.schema.target_column]\n",
    "                model_factory = ModelFactory(model_config_path=model_config_file_path)\n",
    "                best_model = model_factory.get_best_model(X=to_train_features, y=to_train_target, base_accuracy=base_accuracy)\n",
    "                logger.info(f\"Best model found on training dataset: {best_model}\")\n",
    "\n",
    "                logger.info(\"Extracting trained model list.\")\n",
    "                grid_searched_best_model_list: List[GridSearchedBestModel] = model_factory.grid_searched_best_model_list\n",
    "\n",
    "                model_list = [model.best_model for model in grid_searched_best_model_list]\n",
    "                logger.info(f\"Model list: {model_list} , {len(model_list)}\")\n",
    "                cluster_name = f\"cluster_{cluster}\"\n",
    "                report_dir = os.path.join(model_report_dir, cluster_name)\n",
    "                logger.info(\"Evaluation all trained model on training and testing dataset both\")\n",
    "                metric_info: MetricEvalArtifact = evaluate_classification_model(estimators=model_list, X_train=to_train_features,\n",
    "                                                                            y_train=to_train_target, X_test=to_test_features, \n",
    "                                                                            y_test=to_test_target,\n",
    "                                                                            base_accuracy=base_accuracy , report_dir=report_dir,\n",
    "                                                                            eval_difference =eval_difference  , eval_param = eval_param ,\n",
    "                                                                            experiment_id= experiment_id )\n",
    "                logger.info(f\"Metric info: {metric_info}\")\n",
    "        \n",
    "                model_file_name = f\"{metric_info.best_model_name}.pkl\"\n",
    "                model_file_path = os.path.join(report_dir, \"Model\", model_file_name)\n",
    "                model_object = metric_info.best_model\n",
    "                cluster_model_dict[cluster] = model_file_path\n",
    "                save_bin(data = model_object, path = model_file_path)\n",
    "                \n",
    "            logger.info(\"Best found model on both training and testing dataset.\")\n",
    "\n",
    "            \n",
    "\n",
    "            prediction_model = EstimatorModel(preprocessing_object=self.preprocessing_obj, trained_model_dict=cluster_model_dict)\n",
    "            cluster_report_dir = os.path.join(model_report_dir, \"cluster_custom_model\")\n",
    "            cluster_model_path = os.path.join(cluster_report_dir, \"cluster_custom_model.pkl\")\n",
    "            save_bin(data=prediction_model , path = cluster_model_path)\n",
    "            \n",
    "            clustered_model_list = [prediction_model , base_predictor]\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = self.get_validated_data_to_test()\n",
    "\n",
    "            metric_info = evaluate_classification_model(estimators=clustered_model_list, X_train=X_train,\n",
    "                                                                            y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "                                                                            report_dir=cluster_report_dir,                                              \n",
    "                                                                            base_accuracy=base_accuracy ,   eval_difference =eval_difference  , eval_param = eval_param ,\n",
    "                                                                            experiment_id= experiment_id )\n",
    "            logger.info(f\"Saving model at path: {trained_model_file_path}\")\n",
    "            save_object(file_path=trained_model_file_path, obj=prediction_model)\n",
    "\n",
    "            model_trainer_artifact = ModelTrainerArtifact(is_trained=True, message=\"Model Trained successfully\",\n",
    "                                                          trained_model_file_path=trained_model_file_path,\n",
    "                                                          train_f1=metric_info.test_f1,\n",
    "                                                          test_f1=metric_info.test_f1,\n",
    "                                                          train_precision=metric_info.train_precision,\n",
    "                                                          test_precision=metric_info.test_precision,\n",
    "                                                          model_accuracy=metric_info.model_accuracy)\n",
    "\n",
    "            logger.info(f\"Model Trainer Artifact: {model_trainer_artifact}\")\n",
    "            return model_trainer_artifact\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            raise AppException(e, sys) from e\n",
    "\n",
    "    def __del__(self):\n",
    "        logger.info(f\"{'>>' * 30}Model trainer log completed.{'<<' * 30} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creditcard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
