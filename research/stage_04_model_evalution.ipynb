{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MetricEvalArtifact' from 'CreditCard.entity.artifact_entity' (C:\\Users\\princ\\OneDrive\\Desktop\\project-python\\creditcard\\src\\CreditCard\\entity\\artifact_entity.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m \u001b[39mimport\u001b[39;00m AppException\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m logger , get_current_time_stamp\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mDatabase\u001b[39;00m \u001b[39mimport\u001b[39;00m MongoDB\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_object , read_yaml , save_object, evaluate_classification_model\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mentity\u001b[39;00m \u001b[39mimport\u001b[39;00m MetricReportArtifact , ModelEvaluationArtifact , ModelEvaluationConfig\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\project-python\\creditcard\\src\\CreditCard\\Database\\__init__.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m \u001b[39mimport\u001b[39;00m AppException\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m logger\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m read_yaml\n\u001b[0;32m     11\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMongoDB\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"class for mongo db operations\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\project-python\\creditcard\\src\\CreditCard\\utils\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_eval\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluate_classification_model\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\project-python\\creditcard\\src\\CreditCard\\utils\\model_eval.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbox\u001b[39;00m \u001b[39mimport\u001b[39;00m ConfigBox\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mentity\u001b[39;00m \u001b[39mimport\u001b[39;00m MetricEvalArtifact, MetricReportArtifact\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m \u001b[39mimport\u001b[39;00m AppException\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m logger\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\project-python\\creditcard\\src\\CreditCard\\entity\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mentity\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39martifact_entity\u001b[39;00m \u001b[39mimport\u001b[39;00m (DataIngestionArtifact,\n\u001b[0;32m      2\u001b[0m                                                DataTransformationArtifact,\n\u001b[0;32m      3\u001b[0m                                                DataValidationArtifact,\n\u001b[0;32m      4\u001b[0m                                                MetricEvalArtifact,\n\u001b[0;32m      5\u001b[0m                                                MetricReportArtifact,\n\u001b[0;32m      6\u001b[0m                                                ModelTrainerArtifact , ModelEvaluationArtifact)\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mentity\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig_entity\u001b[39;00m \u001b[39mimport\u001b[39;00m (DataIngestionConfig,\n\u001b[0;32m      8\u001b[0m                                              DataTransformationConfig,\n\u001b[0;32m      9\u001b[0m                                              DataValidationConfig,\n\u001b[0;32m     10\u001b[0m                                              ModelTrainerConfig,\n\u001b[0;32m     11\u001b[0m                                              TrainingPipelineConfig , ModelEvaluationConfig)\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mCreditCard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mentity\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcustom_model_entity\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseModel, EstimatorModel\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MetricEvalArtifact' from 'CreditCard.entity.artifact_entity' (C:\\Users\\princ\\OneDrive\\Desktop\\project-python\\creditcard\\src\\CreditCard\\entity\\artifact_entity.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from CreditCard.exception import AppException\n",
    "from CreditCard.logging import logger , get_current_time_stamp\n",
    "from CreditCard.Database import MongoDB\n",
    "from CreditCard.utils import load_object , read_yaml , save_object, evaluate_classification_model\n",
    "from CreditCard.entity import MetricReportArtifact , ModelEvaluationArtifact , ModelEvaluationConfig\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "\n",
    "    def __init__(self , model_evaluation_config : ModelEvaluationConfig):\n",
    "        try:\n",
    "            logger.info(f\"{'>>' * 30}Model Evaluation log started.{'<<' * 30} \")\n",
    "            self.model_evaluation_config = model_evaluation_config\n",
    "            train_collections = self.model_evaluation_config.data_validated_train_collection\n",
    "            train_connection = MongoDB(train_collections, drop_collection=False)\n",
    "            self.train_df = train_connection.find_many_as_df()\n",
    "            test_collections = self.model_evaluation_config.data_validated_test_collection\n",
    "            test_connection = MongoDB(test_collections, drop_collection=False)\n",
    "            self.test_df = test_connection.find_many_as_df()\n",
    "            self.schema = read_yaml(path_to_yaml = Path(self.model_evaluation_config.schema_file_path ))\n",
    "        except Exception as e:\n",
    "            raise AppException(e, sys) from e\n",
    "        \n",
    "    def get_model_list(self , trained_model_dir_path):\n",
    "        model_list = [load_object(file_path= Path(os.path.join(trained_model_dir_path , model_file))) for model_file in os.listdir(trained_model_dir_path)]\n",
    "        return model_list\n",
    "        \n",
    "\n",
    "    def initiate_model_evaluation(self) -> ModelEvaluationArtifact:\n",
    "        try:\n",
    "            trained_model_dir = self.model_trainer_artifact.trained_model_dir\n",
    "            \n",
    "            model_list = self.get_model_list(trained_model_dir)\n",
    "            target_column_name= self.schema.target_column\n",
    "            x_train = self.train_df.drop(columns=[target_column_name ], axis=1)\n",
    "            y_train = self.train_df[target_column_name]\n",
    "            x_test = self.test_df.drop(columns=[target_column_name], axis=1)\n",
    "            y_test = self.test_df[target_column_name]\n",
    "            \n",
    "            report_dir = self.model_evaluation_config.report_dir\n",
    "            base_accuracy = self.model_evaluation_config.base_accuracy\n",
    "            eval_difference = self.model_evaluation_config.eval_difference\n",
    "            eval_param = self.model_evaluation_config.eval_param\n",
    "            \n",
    "            metric_report_artifact : MetricReportArtifact = evaluate_classification_model( x_train_eval= x_train , \n",
    "                                                                    y_train= y_train,\n",
    "                                                                    x_test_eval = x_test,\n",
    "                                                                    y_test =y_test,\n",
    "                                                                    base_accuracy = base_accuracy,\n",
    "                                                                    report_dir = report_dir ,\n",
    "                                                                    eval_difference =eval_difference,\n",
    "                                                                    estimators = model_list,\n",
    "                                                                    eval_param = eval_param,\n",
    "                                                                    experiment_id= \"model_eval\")\n",
    "            \n",
    "            logger.info(f\"Model evaluation completed. model metric artifact: {metric_report_artifact}\")\n",
    "\n",
    "            if metric_report_artifact.model_obj  is  None:\n",
    "                response = ModelEvaluationArtifact(is_model_accepted=False,\n",
    "                                                   evaluated_model_path=None\n",
    "                                                   )\n",
    "                logger.info(response)\n",
    "                return response\n",
    "\n",
    "            else:\n",
    "                logger.info(\"Trained model is found\")\n",
    "                eval_model_dir = self.model_evaluation_config.eval_model_dir\n",
    "                model_file_name = f\"{metric_report_artifact.model_name}.pkl\"\n",
    "                eval_model_path = os.path.join(eval_model_dir, model_file_name)\n",
    "                save_object(obj=metric_report_artifact.model_obj ,file_path=Path(eval_model_path))\n",
    "                \n",
    "                model_evaluation_artifact = ModelEvaluationArtifact(evaluated_model_path=eval_model_path,\n",
    "                                                                    is_model_accepted=False)\n",
    "            return model_evaluation_artifact\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            raise AppException(e, sys) from e\n",
    "\n",
    "    def __del__(self):\n",
    "        logger.info(f\"{'=' * 20}Model Evaluation log completed.{'=' * 20} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creditcard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
